{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from jovis_model.config import Config\n",
    "from run import ModelRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omnious/workspace/jovis/jovis-model/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjovis_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmclip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPModel\n",
      "File \u001b[0;32m~/workspace/jovis/jovis-model/jovis_model/models/llm/mclip.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from PIL import Image\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mclip\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer, PretrainedConfig, PreTrainedModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjovis_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "File \u001b[0;32m~/workspace/jovis/jovis-model/.venv/lib/python3.10/site-packages/clip/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/workspace/jovis/jovis-model/.venv/lib/python3.10/site-packages/clip/clip.py:94\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the names of available CLIP models\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_MODELS\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, device: Union[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, jit: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, download_root: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a CLIP model\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _MODELS:\n",
      "File \u001b[0;32m~/workspace/jovis/jovis-model/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:141\u001b[0m, in \u001b[0;36mis_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m device_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# The default availability inspection never throws and returns 0 if the driver is missing or can't\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# API via `cuInit`\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_getDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from jovis_model.models.llm.mclip import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"pkg\": \"llm\",\n",
    "    \"task\": \"mclip\",\n",
    "    \"use_hf_model\": True,\n",
    "    \"params\": {\n",
    "        \"hf_name\": \"M-CLIP/XLM-Roberta-Large-Vit-B-32\"\n",
    "    }\n",
    "}\n",
    "config = Config(**params)\n",
    "model = CLIPModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:34<00:00, 10.3MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.inference([\"안녕하세요\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./jovis_model/_db/llm/multimodel_test/export_data_skb.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://flush-image-prod.s3.ap-northeast-2.amazonaws.com/01H5S0JRNJEDPD3KX414FBPH2K/693669.webp'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"693669\"][\"s3_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/omnious/workspace/jovis/jovis-model/https:/flush-image-prod.s3.ap-northeast-2.amazonaws.com/01H5S0JRNJEDPD3KX414FBPH2K/693669.webp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m693669\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/jovis/jovis-model/.venv/lib/python3.10/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/omnious/workspace/jovis/jovis-model/https:/flush-image-prod.s3.ap-northeast-2.amazonaws.com/01H5S0JRNJEDPD3KX414FBPH2K/693669.webp'"
     ]
    }
   ],
   "source": [
    "image = Image.open(data[\"693669\"][\"s3_url\"]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     \"pkg\": \"llm\",\n",
    "#     \"task\": \"chat\",\n",
    "#     \"use_hf_model\": True,\n",
    "#     \"params\": {\n",
    "#         \"hf_name\": \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    \"pkg\": \"llm\",\n",
    "    \"task\": \"bedrock\",\n",
    "    \"use_hf_model\": False,\n",
    "    \"params\": {\n",
    "    }\n",
    "}\n",
    "config = Config(**params)\n",
    "runner = ModelRunner(\n",
    "    config=config,\n",
    "    mode=\"inference\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs = [\n",
    "    {\"role\": \"user\", \"content\": \"What is you name?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = runner.run(\n",
    "    sample_inputs=dialogs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InternVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"pkg\": \"llm\",\n",
    "    \"task\": \"internvl\",\n",
    "    \"use_hf_model\": True,\n",
    "    \"params\": {\n",
    "        \"hf_name\": \"OpenGVLab/InternVL-Chat-V1-5\",\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    "}\n",
    "config = Config(**params)\n",
    "runner = ModelRunner(\n",
    "    config=config,\n",
    "    mode=\"inference\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omnious/workspace/jovis/jovis-model/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"pkg\": \"llm\",\n",
    "    \"task\": \"sentence_embedding\",\n",
    "    \"use_hf_model\": True,\n",
    "    \"params\": {\n",
    "        \"hf_name\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    }\n",
    "}\n",
    "config = Config(**params)\n",
    "runner = ModelRunner(\n",
    "    config=config,\n",
    "    mode=\"inference\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"눈의여왕에서 김지원이 사용한 핑크색 가방의 디자인은 어떤 거야?\",\n",
    "    \"요즘 방영 중인 눈의여왕 드라마에서 주인공이 입고 나온 스타일의 청바지를 찾아줘.\",\n",
    "    \"미스터 선샤인 에서 주인공이 입은 모자의 디자인을 찾고 있어요.\",\n",
    "    \"최근 예능 프로그램에서 나온 배우 김지원의 데님 팬츠를 찾고 있어요.\",\n",
    "    \"지난 주 “눈의여왕” 드라마에서 주인공이 입은 외투를 찾아줄 수 있어?\",\n",
    "    \"어느 멋진 날에서 MC가 입은 슈트의 색상을 알려주세요.\",\n",
    "    \"최근에 방영된 “마이데몬”에서 주인공이 입은 신발의 모델명은 무엇이야?\",\n",
    "    \"어제 방송된 “눈의여왕”에서 여주인공이 입은 원피스의 색상은?\",\n",
    "    \"요즘 인기 있는 드라마에서 주인공이 입은 아우터를 알려줘.\",\n",
    "    \"최근 방영된 “마이데몬”에서 주인공이 입은 액세서리가 돋보이는데, 어디서 구할 수 있어?\",\n",
    "    \"어제 방송된 드라마에서 여주인공이 입은 셔츠의 디자인을 찾아줄래?\",\n",
    "    \"최신 드라마에서 남자 주인공이 입은 코트를 찾아줘.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = runner.run(samples[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/omnious/workspace/jovis/jovis-model/outputs/skb/descriptions_v2.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    descriptions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19180/19180 [02:58<00:00, 107.20it/s]\n"
     ]
    }
   ],
   "source": [
    "pids = []\n",
    "embeddings = []\n",
    "for pid, description in tqdm(descriptions.items()):\n",
    "    pids.append(pid)\n",
    "    embeddings.append(runner.run([description]).detach().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_faiss_index(\n",
    "    embeddings=embeddings,\n",
    "    save_path=\"/home/omnious/workspace/jovis/jovis-model/outputs/skb\",\n",
    "    save_name=\"descriptions_v2\",\n",
    "    pids=pids\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
